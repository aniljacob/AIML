{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniljacob/AIML/blob/main/Demo_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWR22OAlTBlU"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zKf9UCTFQW"
      },
      "source": [
        "### Not for Grading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcOK1wB_sCou"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yETAUjEur-48"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "*   Use NLTK package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nIAXZNu7WlT"
      },
      "source": [
        "## Background\n",
        "\n",
        "\n",
        "### What is NLTK? \n",
        "\n",
        "\n",
        "The Natural Language Toolkit (NLTK)is a package in python that provides libraries for different text processing techniques, such as classification, tokenization, stemming and tagging.\n",
        "\n",
        "**NLTK corpus**\n",
        "\n",
        "\n",
        "**Punkt:** This tokenizer divides a text into a list of sentences to build a model for abbreviation words, collocations and words with sentences. \n",
        "\n",
        "\n",
        "\n",
        "**Wordnet**: WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets).\n",
        "\n",
        "\n",
        "\n",
        "**averaged_perceptron_tagger**: It is used for tagging words with their parts of speech (POS)\n",
        "\n",
        "**tagset:** The tagset consists of the following tags:\n",
        "\n",
        "VB - verbs (all tenses and modes)\n",
        "\n",
        "NN - nouns (common and proper)\n",
        "\n",
        "PRON - pronouns\n",
        "\n",
        "ADJ - adjectives\n",
        "\n",
        "ADV - adverbs\n",
        "\n",
        "ADP - adpositions (prepositions and postpositions)\n",
        "\n",
        "CONJ - conjunctions\n",
        "\n",
        "DET - determiners\n",
        "\n",
        "NUM - cardinal numbers\n",
        "\n",
        "PRT - particles or other function words\n",
        "\n",
        "IN -  preposition/subordinating conjunction\n",
        "\n",
        "NNS - noun plural ‘desks’\n",
        "\n",
        "JJ  - adjective ‘big’\n",
        "\n",
        "VBP - verb, sing. present, non-3d take\n",
        "\n",
        "DT - determiner\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFdSLnnHTMxC"
      },
      "source": [
        "### Setup Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOFz5-jgTQ3S"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2100706\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWJFJRoTTQ6a"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"7702777831\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRvlfkw5rV_Q",
        "outputId": "3f985519-7432-422f-8380-9f8a7f572c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"Demo_NLTK\" #name of the notebook\n",
        "Answer = \"Ungraded\"\n",
        "\n",
        "def setup():\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/shakespeare.txt\")\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    \n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aiml.iiith.talentsprint.com/notebook_submissions\")\n",
        "        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "      return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "    \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2100706&recordId=15703\"></script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Setup completed successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQwxoAUp3M6n"
      },
      "source": [
        "### Read the file for pre-processing\n",
        "\n",
        "Using Shakespeare text file which is extracted in a webscraping notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEPqGsuew4mw"
      },
      "source": [
        "f = open(\"shakespeare.txt\", \"r\")\n",
        "\n",
        "# Reading the file\n",
        "text = f.read()\n",
        "f.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGOnHkKZLNYh"
      },
      "source": [
        "### Normalizing Text\n",
        "\n",
        "1. Converting all letters to lower case\n",
        "2. Removing newline characters '\\n' from given text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEpE9x9FOmzh"
      },
      "source": [
        "# Converting all letters to lower case\n",
        "text = text.lower()\n",
        "\n",
        "# Removing new line characters\n",
        "text = text.replace('\\n',\" \")\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdOS0uHlFWNV"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is a way to split the text into tokens. These tokens could be paragraphs, sentences, or individual words. These tokens are useful for finding such patterns and considered as a base step for stemming and Lemmatization\n",
        "\n",
        "NLTK comes with many corpora, downloading a pre-trained Punkt tokenizer to perform tokenization, and a wordnet to perform lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzvsA_81GZjS"
      },
      "source": [
        "# Importing nltk package\n",
        "import nltk\n",
        "\n",
        "# Downloading punkt from NLTK to perform sentence tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Downloading wordnet from NLTK to perform Lemmatization\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-78Q2AbvOnoB"
      },
      "source": [
        "### Sentence Level Tokenization\n",
        "\n",
        "Sentence tokenization is the process of splitting text into individual sentences.\n",
        "\n",
        "For example :\n",
        "\n",
        "**Input :** `Sun rises in the east. Sun sets in the west.`\n",
        "\n",
        "**Output:** `['Sun rises in the east.', 'Sun sets in the west.']`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6594Cua5OzSK"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# sent_tokenize() is to split a document or paragraph into sentences\n",
        "sen_token = sent_tokenize(text) \n",
        "\n",
        "# Finding the length of the sentances\n",
        "len(sen_token) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANCIDJB7zWI7"
      },
      "source": [
        "# Printing 10th sentence\n",
        "print(sen_token[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsqxaFZhZ87K"
      },
      "source": [
        "Average sentence length  gives the maximum words in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er0wPUprZ8im"
      },
      "source": [
        "word_count = [len(sent.split()) for i, sent in enumerate(sen_token)]\n",
        "print(\"Number of words in each sentence:\", word_count)\n",
        "\n",
        "Avg_Sent = sum(word_count)//len(word_count)\n",
        "print(\"Average number of words used in each sentence: \", Avg_Sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaeN09igYdRG"
      },
      "source": [
        "### Word Level Tokenization\n",
        "\n",
        "Word tokenization is the process of splitting text into words.\n",
        " \n",
        "For example\n",
        "\n",
        "**Input:** ```\"Sun rises in the east. Sun sets in the west.\"```\n",
        "\n",
        "**Output:** ```[\"Sun\" ,\"rises\" ,\"in\", \"the\", \"east\", \".\", \"Sun\", \"sets\", \"in\", \"the\", \"west\",  \".\"]```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM_NJE5tFHxT"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# word_tokenize() method to split a sentence into tokens or words\n",
        "wtokens = word_tokenize(text)\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va7XAlbvCxFV"
      },
      "source": [
        "### Removing Punctuations\n",
        "\n",
        "To remove punctuations from the text, apply translation function\n",
        "\n",
        "maketrans() takes 3 arguments and returns a translation table usable for str.translate()\n",
        "\n",
        "\n",
        "For example:\n",
        "\n",
        "**Input:** ```[\"Sun\" ,\"rises\" ,\"in\", \"the\", \"east\", \".\", \"Sun\", \"sets\", \"in\", \"the\", \"west\",  \".\"]```\n",
        "\n",
        "**Output:** ```[\"Sun\" ,\"rises\" ,\"in\", \"the\", \"east\", \"Sun\", \"sets\", \"in\", \"the\", \"west\"]```\n",
        "\n",
        "Punctuations are removed in the above example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZRg7E9zlP2u"
      },
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcrf7LVeFJQq"
      },
      "source": [
        "If a word itself is a punctuation, then ignore the word. It will not be appened in the words list.\n",
        "\n",
        "If there is a punctuation within the word, then use str.maketrans to remove the punctuation within the word and append the word in the words list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCKV5BIwR-a7"
      },
      "source": [
        "remove_punctuation = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "words = []\n",
        "\n",
        "for token in wtokens: \n",
        "  if token in string.punctuation:\n",
        "    pass # If token itself is a punctuation mark (#, $, %, &) then pass / ignore\n",
        "  else:\n",
        "    words.append(token.translate(remove_punctuation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jFCG5hYG9sz"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming is the process of reducing a word to its root stem. \n",
        "\n",
        "For example:\n",
        "\n",
        "**Input:**  ```['Sun', 'rises', 'in', 'the', 'east', '.', 'Sun', 'sets', 'in', 'the', 'west', '.']```\n",
        "\n",
        "**Output:** ```['sun', 'rise', 'in', 'the', 'east', '.', 'sun', 'set', 'in', 'the', 'west', '.']```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4m-19AHoTZB"
      },
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "stem = [porter.stem(i) for i in words]\n",
        "print(stem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BUfwJ8yHuNu"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "Lemmatization is similar to stemming but it brings context to the words.\n",
        "\n",
        "For example:\n",
        "\n",
        "* rocks : rock\n",
        "* corpora : corpus\n",
        "* better : good"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD2yqJI6HsJi"
      },
      "source": [
        "lemma = nltk.WordNetLemmatizer()\n",
        "lemmatizer = [lemma.lemmatize(i) for i in words]\n",
        "print(lemmatizer[0:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O37sarOcalJ"
      },
      "source": [
        "### Parts of Speech\n",
        "\n",
        "\n",
        "Given any sentence, classify each word as a noun, verb, conjunction, or any other class of words.\n",
        "\n",
        "When there are hundreds of thousands of sentences, this is a large and tedious task.\n",
        "\n",
        "NLTK provides **averaged_perceptron_tagger** for tagging words with their parts of speech (POS).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPSoIDtpg2WG"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfCknvySTH1E"
      },
      "source": [
        "**nltk.pos_tag** attaches a part of speech tag to each word in the given list of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7JiEUnHcaNq"
      },
      "source": [
        "# Get parts of speech tags for the first 10 words from wtokens\n",
        "parts_of_speech = nltk.pos_tag(words[:10])\n",
        "print(parts_of_speech)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5zlHD9WnNR6"
      },
      "source": [
        "To know what is DT, JJ, or any other tags\n",
        "\n",
        "Download the tagsets and get the details of each tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq8dZNtWOzyB"
      },
      "source": [
        "nltk.download('tagsets')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7W_0zc4QQpk"
      },
      "source": [
        "# Get the details of DT tag\n",
        "nltk.help.upenn_tagset('DT') # Replace DT with any other tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FouUAfWzTAPA"
      },
      "source": [
        "To get the nouns count from the first 10 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9MkCPfsTjKL"
      },
      "source": [
        "top10_words = [pos for pos in parts_of_speech if pos[1] == 'NN']\n",
        "\n",
        "# Count of NN words\n",
        "print(len(top10_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UjYDKMsI7fR"
      },
      "source": [
        "#### Removing Stopwords\n",
        "\n",
        "Download all the stopwords from the NLTK package using nltk.download('stopwords') and then remove the unwanted words from the given list of words\n",
        "\n",
        "few stopwords from the NLTK package are  “the”, “a”, “an”, “in”, \"at\", \"of\".\n",
        "\n",
        "**Example:**\n",
        "\n",
        "**Input:** `\"Sun rises in the east Sun sets in the west\"`\n",
        "\n",
        "**Output:** `[\"Sun\", \"rises\", \"east\", \"Sun\", \"sets\", \"west\"]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjpPR_E_Iuob"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords  \n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4DqUaU1JEpg"
      },
      "source": [
        "# Iterate over all the words and appending the words which are not there in the stopwords\n",
        "pre_processed = []\n",
        "for i, word in enumerate(words):\n",
        "  if word not in stop_words:\n",
        "    pre_processed.append(word)\n",
        "    \n",
        "print(pre_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMwDBNCeJdKI"
      },
      "source": [
        "#### Frequency Distribution\n",
        "\n",
        "A frequency distribution is used to record the frequency of each word type from given text\n",
        "\n",
        "NLTK provides FreqDist function to get the count of each word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hqLfjMEJgjv"
      },
      "source": [
        "# Generating word and word_count in (key, value) pairs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Word_Frequency = nltk.FreqDist(pre_processed)\n",
        "\n",
        "print(Word_Frequency.keys())\n",
        "print(Word_Frequency.values())\n",
        "\n",
        "# Plot word frequency for 25 words\n",
        "Word_Frequency.plot(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC2rt1ZxrgC7"
      },
      "source": [
        "## Please answer the questions below to complete the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cTetkuegP7d"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFQw0ddId_Ej"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CXztFuygSBG",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook  { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id =return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}